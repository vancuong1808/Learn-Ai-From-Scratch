"""
TOKYO HOUSE PRICE PREDICTION - ÄÆ N GIáº¢N CHO NGÆ¯á»œI Má»šI Há»ŒC
========================================================

Pipeline hoÃ n chá»‰nh Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  á»Ÿ Tokyo vá»›i Linear Regression:
âœ… Load vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u
âœ… PhÃ¢n tÃ­ch vÃ  xá»­ lÃ½ missing data (MCAR, MNAR, MAR)
âœ… Feature engineering Ä‘Æ¡n giáº£n
âœ… Training Linear Regression
âœ… ÄÃ¡nh giÃ¡ vÃ  dá»± Ä‘oÃ¡n

TÃ¡c giáº£: AI Assistant
NgÃ y: July 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Thiáº¿t láº­p style cho plots
plt.style.use('default')
sns.set_palette("husl")

print("ğŸ  TOKYO HOUSE PRICE PREDICTION - SIMPLE VERSION")
print("=" * 60)

# ========================================
# BÆ¯á»šC 1: LOAD VÃ€ KHÃM PHÃ Dá»® LIá»†U
# ========================================
print("\nğŸ“ BÆ¯á»šC 1: Load vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u")
print("-" * 40)

try:
    # Load datasets
    train_data = pd.read_csv("train.csv")
    test_data = pd.read_csv("test.csv")
    sample_submission = pd.read_csv("sample_submission.csv")
    
    print("âœ… Load dá»¯ liá»‡u thÃ nh cÃ´ng!")
    print(f"   ğŸ“Š Train data: {train_data.shape[0]:,} rows, {train_data.shape[1]} columns")
    print(f"   ğŸ“Š Test data: {test_data.shape[0]:,} rows, {test_data.shape[1]} columns")
    
except FileNotFoundError as e:
    print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file - {e}")
    print("ğŸ’¡ HÃ£y Ä‘áº£m báº£o cÃ¡c file train.csv, test.csv, sample_submission.csv á»Ÿ cÃ¹ng thÆ° má»¥c")
    exit(1)

# ThÃ´ng tin cÆ¡ báº£n vá» dataset
print(f"\nğŸ” ThÃ´ng tin dataset:")
numerical_cols = train_data.select_dtypes(include=[np.number]).columns
categorical_cols = train_data.select_dtypes(include=['object']).columns

print(f"   ğŸ”¢ Numerical features: {len(numerical_cols)}")
print(f"   ğŸ“ Categorical features: {len(categorical_cols)}")

# PhÃ¢n tÃ­ch target variable
target = train_data['Price_JPY']
print(f"\nğŸ¯ PhÃ¢n tÃ­ch biáº¿n má»¥c tiÃªu (Price_JPY):")
print(f"   ğŸ’° GiÃ¡ trung bÃ¬nh: {target.mean():,.0f} JPY")
print(f"   ğŸ’° GiÃ¡ trung vá»‹: {target.median():,.0f} JPY")
print(f"   ğŸ’° GiÃ¡ tháº¥p nháº¥t: {target.min():,.0f} JPY")
print(f"   ğŸ’° GiÃ¡ cao nháº¥t: {target.max():,.0f} JPY")
print(f"   ğŸ“ˆ Äá»™ lá»‡ch chuáº©n: {target.std():,.0f} JPY")

# Visualize target distribution
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.hist(target, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('PhÃ¢n phá»‘i giÃ¡ nhÃ ')
plt.xlabel('GiÃ¡ (JPY)')
plt.ylabel('Táº§n suáº¥t')

plt.subplot(1, 3, 2)
plt.hist(np.log1p(target), bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
plt.title('PhÃ¢n phá»‘i Log(GiÃ¡)')
plt.xlabel('Log(GiÃ¡)')
plt.ylabel('Táº§n suáº¥t')

plt.subplot(1, 3, 3)
plt.boxplot(target)
plt.title('Box Plot - PhÃ¡t hiá»‡n outliers')
plt.ylabel('GiÃ¡ (JPY)')

plt.tight_layout()
plt.show()

# ========================================
# BÆ¯á»šC 2: PHÃ‚N TÃCH MISSING DATA
# ========================================
print("\n" + "=" * 60)
print("ğŸ” BÆ¯á»šC 2: PhÃ¢n tÃ­ch Missing Data")
print("=" * 60)

def analyze_missing_data(df, name="Dataset"):
    """PhÃ¢n tÃ­ch chi tiáº¿t missing data"""
    print(f"\nğŸ“‹ PhÃ¢n tÃ­ch Missing Data - {name}:")
    
    missing_count = df.isnull().sum()
    missing_percent = (missing_count / len(df)) * 100
    
    missing_summary = pd.DataFrame({
        'Column': df.columns,
        'Missing_Count': missing_count,
        'Missing_Percent': missing_percent,
        'Data_Type': df.dtypes
    })
    
    # Chá»‰ hiá»ƒn thá»‹ columns cÃ³ missing data
    missing_only = missing_summary[missing_summary['Missing_Count'] > 0].sort_values(
        'Missing_Count', ascending=False
    )
    
    if len(missing_only) == 0:
        print("   âœ… KhÃ´ng cÃ³ missing data!")
        return missing_only
    
    print(f"   ğŸ“Š Tá»•ng quan:")
    print(f"      - Columns cÃ³ missing: {len(missing_only)}/{len(df.columns)}")
    print(f"      - Tá»•ng missing values: {missing_count.sum():,}")
    
    print(f"\n   ğŸ“ˆ Chi tiáº¿t:")
    for _, row in missing_only.head(10).iterrows():
        print(f"      - {row['Column']}: {row['Missing_Count']:,} ({row['Missing_Percent']:.1f}%) - {row['Data_Type']}")
    
    return missing_only

# PhÃ¢n tÃ­ch missing data cho cáº£ train vÃ  test
missing_train = analyze_missing_data(train_data, "Train")
missing_test = analyze_missing_data(test_data, "Test")

# Giáº£i thÃ­ch cÃ¡c loáº¡i Missing Data Mechanisms
print(f"\nğŸ§  CÃC LOáº I MISSING DATA MECHANISMS:")
print(f"   ğŸ’¡ MCAR (Missing Completely At Random):")
print(f"      - Missing hoÃ n toÃ n ngáº«u nhiÃªn, khÃ´ng liÃªn quan Ä‘áº¿n báº¥t ká»³ biáº¿n nÃ o")
print(f"      - VÃ­ dá»¥: Lá»—i ká»¹ thuáº­t khi thu tháº­p dá»¯ liá»‡u")
print(f"   ğŸ’¡ MAR (Missing At Random):")
print(f"      - Missing phá»¥ thuá»™c vÃ o cÃ¡c biáº¿n khÃ¡c cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c")
print(f"      - VÃ­ dá»¥: NgÆ°á»i tráº» Ã­t khai bÃ¡o thu nháº­p hÆ¡n ngÆ°á»i giÃ ")
print(f"   ğŸ’¡ MNAR (Missing Not At Random):")
print(f"      - Missing phá»¥ thuá»™c vÃ o chÃ­nh giÃ¡ trá»‹ bá»‹ missing")
print(f"      - VÃ­ dá»¥: NhÃ  chÆ°a renovation thÃ¬ YearRenovated = 0")

# PhÃ¢n tÃ­ch cá»¥ thá»ƒ YearRenovated
if 'YearRenovated' in train_data.columns:
    zero_renovation = (train_data['YearRenovated'] == 0).sum()
    missing_renovation = train_data['YearRenovated'].isnull().sum()
    
    print(f"\nğŸ  PhÃ¢n tÃ­ch Ä‘áº·c biá»‡t - YearRenovated:")
    print(f"   ğŸ“Š GiÃ¡ trá»‹ 0 (chÆ°a renovation): {zero_renovation:,} ({zero_renovation/len(train_data)*100:.1f}%)")
    print(f"   ğŸ“Š GiÃ¡ trá»‹ missing (NaN): {missing_renovation:,}")
    print(f"   ğŸ’­ Káº¿t luáº­n: ÄÃ¢y lÃ  trÆ°á»ng há»£p MNAR vÃ¬ 0 cÃ³ Ã½ nghÄ©a 'chÆ°a Ä‘Æ°á»£c renovation'")

# ========================================
# BÆ¯á»šC 3: Xá»¬ LÃ MISSING DATA
# ========================================
print("\n" + "=" * 60)
print("ğŸ”§ BÆ¯á»šC 3: Xá»­ lÃ½ Missing Data")
print("=" * 60)

# Táº¡o báº£n sao Ä‘á»ƒ xá»­ lÃ½
train_clean = train_data.copy()
test_clean = test_data.copy()

print(f"ğŸ“ CHIáº¾N LÆ¯á»¢C Xá»¬ LÃ:")

# 1. Xá»­ lÃ½ YearRenovated (MNAR case)
if 'YearRenovated' in train_clean.columns:
    print(f"\n1ï¸âƒ£ Xá»­ lÃ½ YearRenovated (MNAR):")
    print(f"   - Chuyá»ƒn Ä‘á»•i 0 â†’ NaN (vÃ¬ 0 cÃ³ nghÄ©a lÃ  'chÆ°a renovation')")
    print(f"   - Sau Ä‘Ã³ impute NaN vá»›i median cá»§a cÃ¡c giÃ¡ trá»‹ renovation thá»±c táº¿")
    
    # Chuyá»ƒn 0 thÃ nh NaN
    train_clean['YearRenovated'] = train_clean['YearRenovated'].replace(0, np.nan)
    test_clean['YearRenovated'] = test_clean['YearRenovated'].replace(0, np.nan)

# 2. PhÃ¢n loáº¡i features
numerical_features = train_clean.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = train_clean.select_dtypes(include=['object']).columns.tolist()

# Loáº¡i bá» target variable
if 'Price_JPY' in numerical_features:
    numerical_features.remove('Price_JPY')

print(f"\n2ï¸âƒ£ PhÃ¢n loáº¡i features:")
print(f"   ğŸ”¢ Numerical: {len(numerical_features)} features")
print(f"   ğŸ“ Categorical: {len(categorical_features)} features")

# 3. Impute numerical features vá»›i median
print(f"\n3ï¸âƒ£ Xá»­ lÃ½ Numerical Features (Impute vá»›i Median):")
for col in numerical_features:
    if train_clean[col].isnull().sum() > 0:
        # TÃ­nh median tá»« train set
        median_val = train_clean[col].median()
        
        # Impute cáº£ train vÃ  test
        train_clean[col].fillna(median_val, inplace=True)
        test_clean[col].fillna(median_val, inplace=True)
        
        print(f"   âœ… {col}: imputed with median = {median_val:.2f}")

# 4. Impute categorical features vá»›i mode
print(f"\n4ï¸âƒ£ Xá»­ lÃ½ Categorical Features (Impute vá»›i Mode):")
for col in categorical_features:
    if train_clean[col].isnull().sum() > 0:
        # TÃ­nh mode tá»« train set
        mode_values = train_clean[col].mode()
        mode_val = mode_values[0] if len(mode_values) > 0 else 'Unknown'
        
        # Impute cáº£ train vÃ  test
        train_clean[col].fillna(mode_val, inplace=True)
        test_clean[col].fillna(mode_val, inplace=True)
        
        print(f"   âœ… {col}: imputed with mode = '{mode_val}'")

# Kiá»ƒm tra káº¿t quáº£
remaining_train = train_clean.isnull().sum().sum()
remaining_test = test_clean.isnull().sum().sum()

print(f"\nğŸ¯ Káº¾T QUáº¢ Xá»¬ LÃ MISSING DATA:")
print(f"   âœ… Train: {remaining_train} missing values cÃ²n láº¡i")
print(f"   âœ… Test: {remaining_test} missing values cÃ²n láº¡i")

if remaining_train == 0 and remaining_test == 0:
    print(f"   ğŸ‰ HoÃ n thÃ nh! KhÃ´ng cÃ²n missing data nÃ o.")
else:
    print(f"   âš ï¸ Váº«n cÃ²n missing data - cáº§n kiá»ƒm tra láº¡i!")

# ========================================
# BÆ¯á»šC 4: FEATURE ENGINEERING ÄÆ N GIáº¢N
# ========================================
print("\n" + "=" * 60)
print("âš™ï¸ BÆ¯á»šC 4: Feature Engineering ÄÆ¡n Giáº£n")
print("=" * 60)

def create_simple_features(df):
    """Táº¡o cÃ¡c features má»›i Ä‘Æ¡n giáº£n vÃ  dá»… hiá»ƒu"""
    df = df.copy()
    
    print(f"ğŸ”§ Äang táº¡o features má»›i...")
    
    # 1. Tuá»•i cá»§a tÃ²a nhÃ 
    current_year = 2024
    df['BuildingAge'] = current_year - df['YearBuilt']
    print(f"   âœ… BuildingAge = {current_year} - YearBuilt")
    
    # 2. Sá»‘ nÄƒm ká»ƒ tá»« láº§n renovation cuá»‘i
    df['YearsSinceRenovation'] = current_year - df['YearRenovated']
    # Náº¿u chÆ°a renovation thÃ¬ = tuá»•i tÃ²a nhÃ 
    df['YearsSinceRenovation'].fillna(df['BuildingAge'], inplace=True)
    print(f"   âœ… YearsSinceRenovation = {current_year} - YearRenovated")
    
    # 3. Tá»· lá»‡ diá»‡n tÃ­ch Ä‘áº¥t/sÃ n
    df['LandToFloorRatio'] = df['LandArea_sqm'] / (df['TotalFloorArea_sqm'] + 1e-8)
    print(f"   âœ… LandToFloorRatio = LandArea / TotalFloorArea")
    
    # 4. Diá»‡n tÃ­ch trung bÃ¬nh má»—i phÃ²ng
    df['AreaPerRoom'] = df['TotalFloorArea_sqm'] / (df['RoomCount'] + 1e-8)
    print(f"   âœ… AreaPerRoom = TotalFloorArea / RoomCount")
    
    # 5. Tá»· lá»‡ phÃ²ng ngá»§
    df['BedroomRatio'] = df['BedroomCount'] / (df['RoomCount'] + 1e-8)
    print(f"   âœ… BedroomRatio = BedroomCount / RoomCount")
    
    # 6. Äiá»ƒm tiá»‡n nghi (tá»•ng sá»‘ tiá»‡n Ã­ch cao cáº¥p)
    luxury_features = ['HasGym', 'HasConcierge', 'HasLounge', 'HasGuestRoom']
    df['LuxuryScore'] = df[luxury_features].sum(axis=1)
    print(f"   âœ… LuxuryScore = sum of luxury amenities")
    
    # 7. Äiá»ƒm tiá»‡n nghi cÆ¡ báº£n
    basic_amenities = ['SmartHome', 'CentralAC', 'FloorHeating', 'HasBalcony']
    df['BasicAmenityScore'] = df[basic_amenities].sum(axis=1)
    print(f"   âœ… BasicAmenityScore = sum of basic amenities")
    
    # 8. CÃ³ khÃ´ng gian ngoÃ i trá»i
    outdoor_features = ['HasBalcony', 'HasRooftop', 'HasGarden']
    df['HasOutdoorSpace'] = (df[outdoor_features].sum(axis=1) > 0).astype(int)
    print(f"   âœ… HasOutdoorSpace = cÃ³ Ã­t nháº¥t 1 outdoor feature")
    
    # 9. NhÃ  má»›i hay cÅ©
    df['IsNewBuilding'] = (df['BuildingAge'] <= 10).astype(int)
    print(f"   âœ… IsNewBuilding = 1 if BuildingAge <= 10")
    
    # 10. Äiá»ƒm cháº¥t lÆ°á»£ng tá»•ng thá»ƒ
    df['QualityScore'] = (df['ExteriorCondition'] + df['InteriorCondition']) / 2
    print(f"   âœ… QualityScore = trung bÃ¬nh ExteriorCondition vÃ  InteriorCondition")
    
    return df

# Ãp dá»¥ng feature engineering
print(f"\nğŸš€ Ãp dá»¥ng Feature Engineering:")
train_fe = create_simple_features(train_clean)
test_fe = create_simple_features(test_clean)

# TÃ­nh sá»‘ features má»›i
original_features = train_clean.shape[1]
new_features_count = train_fe.shape[1] - original_features

print(f"\nğŸ“Š Káº¾T QUáº¢ FEATURE ENGINEERING:")
print(f"   ğŸ“ˆ Features ban Ä‘áº§u: {original_features}")
print(f"   ğŸ“ˆ Features sau khi táº¡o má»›i: {train_fe.shape[1]}")
print(f"   âœ¨ Sá»‘ features má»›i: {new_features_count}")

# ========================================
# BÆ¯á»šC 5: CATEGORICAL ENCODING
# ========================================
print("\n" + "=" * 60)
print("ğŸ·ï¸ BÆ¯á»šC 5: Categorical Encoding")
print("=" * 60)

# Láº¥y danh sÃ¡ch categorical features
categorical_features_fe = train_fe.select_dtypes(include=['object']).columns.tolist()

print(f"ğŸ“ Cáº§n encode {len(categorical_features_fe)} categorical features:")
for i, col in enumerate(categorical_features_fe, 1):
    unique_count = train_fe[col].nunique()
    print(f"   {i}. {col}: {unique_count} unique values")

# Label Encoding
print(f"\nğŸ”„ Thá»±c hiá»‡n Label Encoding:")
label_encoders = {}

for col in categorical_features_fe:
    print(f"   ğŸ·ï¸ Encoding {col}...")
    
    # Táº¡o label encoder
    le = LabelEncoder()
    
    # Káº¿t há»£p train vÃ  test Ä‘á»ƒ Ä‘áº£m báº£o consistent encoding
    combined_values = pd.concat([train_fe[col], test_fe[col]]).astype(str)
    le.fit(combined_values)
    
    # Transform cáº£ train vÃ  test
    train_fe[col] = le.transform(train_fe[col].astype(str))
    test_fe[col] = le.transform(test_fe[col].astype(str))
    
    # LÆ°u encoder Ä‘á»ƒ sau nÃ y cÃ³ thá»ƒ decode
    label_encoders[col] = le
    
    print(f"      âœ… {col}: {len(le.classes_)} categories encoded")

print(f"\nâœ… HoÃ n thÃ nh Categorical Encoding!")
print(f"   ğŸ“Š Táº¥t cáº£ features giá» Ä‘Ã£ lÃ  numerical")

# ========================================
# BÆ¯á»šC 6: CHUáº¨N Bá»Š Dá»® LIá»†U CHO MODEL
# ========================================
print("\n" + "=" * 60)
print("ğŸ“¦ BÆ¯á»šC 6: Chuáº©n bá»‹ dá»¯ liá»‡u cho Model")
print("=" * 60)

# TÃ¡ch features vÃ  target
feature_columns = [col for col in train_fe.columns if col != 'Price_JPY']
X = train_fe[feature_columns]
y = train_fe['Price_JPY']
X_test = test_fe[feature_columns]

print(f"ğŸ“Š KÃCH THÆ¯á»šC Dá»® LIá»†U:")
print(f"   ğŸ”¢ Features (X): {X.shape}")
print(f"   ğŸ¯ Target (y): {y.shape}")
print(f"   ğŸ§ª Test features: {X_test.shape}")

# Kiá»ƒm tra khÃ´ng cÃ³ missing data
assert X.isnull().sum().sum() == 0, "âŒ Váº«n cÃ²n missing data trong X!"
assert X_test.isnull().sum().sum() == 0, "âŒ Váº«n cÃ²n missing data trong X_test!"
print(f"   âœ… KhÃ´ng cÃ³ missing data")

# Train-Validation Split
print(f"\nâœ‚ï¸ CHIA TRAIN-VALIDATION:")
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"   ğŸ“Š Train set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"   ğŸ“Š Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)")

# Feature Scaling
print(f"\nâš–ï¸ FEATURE SCALING:")
print(f"   ğŸ”§ Sá»­ dá»¥ng StandardScaler (mean=0, std=1)")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print(f"   âœ… ÄÃ£ scale train set (fit_transform)")
print(f"   âœ… ÄÃ£ scale validation set (transform)")
print(f"   âœ… ÄÃ£ scale test set (transform)")

# ========================================
# BÆ¯á»šC 7: TRAINING MODEL
# ========================================
print("\n" + "=" * 60)
print("ğŸ¤– BÆ¯á»šC 7: Training Linear Regression Model")
print("=" * 60)

print(f"ğŸš€ TRAINING LINEAR REGRESSION:")
print(f"   ğŸ“š Algorithm: Ordinary Least Squares")
print(f"   ğŸ“Š Features: {X_train_scaled.shape[1]}")
print(f"   ğŸ“Š Training samples: {X_train_scaled.shape[0]:,}")

# Khá»Ÿi táº¡o vÃ  train model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

print(f"   âœ… Training hoÃ n thÃ nh!")

# ThÃ´ng tin vá» model
print(f"\nğŸ“‹ THÃ”NG TIN MODEL:")
print(f"   ğŸ”¢ Coefficients: {len(model.coef_):,}")
print(f"   ğŸ¯ Intercept: {model.intercept_:,.0f}")

# Top 5 features quan trá»ng nháº¥t (theo absolute coefficients)
feature_importance = pd.DataFrame({
    'Feature': feature_columns,
    'Coefficient': model.coef_,
    'Abs_Coefficient': np.abs(model.coef_)
}).sort_values('Abs_Coefficient', ascending=False)

print(f"\nâ­ TOP 5 FEATURES QUAN TRá»ŒNG NHáº¤T:")
for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):
    print(f"   {i}. {row['Feature']}: {row['Coefficient']:,.2f}")

# ========================================
# BÆ¯á»šC 8: ÄÃNH GIÃ MODEL
# ========================================
print("\n" + "=" * 60)
print("ğŸ“Š BÆ¯á»šC 8: ÄÃ¡nh giÃ¡ Model")
print("=" * 60)

def calculate_metrics(y_true, y_pred, dataset_name):
    """TÃ­nh toÃ¡n vÃ  hiá»ƒn thá»‹ cÃ¡c metrics"""
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    print(f"\nğŸ“ˆ {dataset_name.upper()} PERFORMANCE:")
    print(f"   ğŸ¯ RMSE: {rmse:,.0f} JPY")
    print(f"   ğŸ¯ MAE: {mae:,.0f} JPY")
    print(f"   ğŸ¯ RÂ² Score: {r2:.4f}")
    
    # Giáº£i thÃ­ch RÂ² score
    if r2 >= 0.8:
        performance = "Xuáº¥t sáº¯c"
    elif r2 >= 0.6:
        performance = "Tá»‘t"
    elif r2 >= 0.4:
        performance = "Trung bÃ¬nh"
    else:
        performance = "Cáº§n cáº£i thiá»‡n"
    
    print(f"   ğŸ’¬ ÄÃ¡nh giÃ¡: {performance}")
    
    return rmse, mae, r2

# Dá»± Ä‘oÃ¡n vÃ  Ä‘Ã¡nh giÃ¡
print(f"ğŸ”® THá»°C HIá»†N Dá»° ÄOÃN:")
y_train_pred = model.predict(X_train_scaled)
y_val_pred = model.predict(X_val_scaled)

# TÃ­nh metrics
train_rmse, train_mae, train_r2 = calculate_metrics(y_train, y_train_pred, "training")
val_rmse, val_mae, val_r2 = calculate_metrics(y_val, y_val_pred, "validation")

# Kiá»ƒm tra overfitting/underfitting
print(f"\nğŸ” PHÃ‚N TÃCH OVERFITTING:")
r2_diff = train_r2 - val_r2
print(f"   ğŸ“Š Train RÂ²: {train_r2:.4f}")
print(f"   ğŸ“Š Validation RÂ²: {val_r2:.4f}")
print(f"   ğŸ“Š ChÃªnh lá»‡ch: {r2_diff:.4f}")

if r2_diff < 0.05:
    print(f"   âœ… Model á»•n Ä‘á»‹nh (khÃ´ng overfitting)")
elif r2_diff < 0.1:
    print(f"   âš ï¸ CÃ³ dáº¥u hiá»‡u overfitting nháº¹")
else:
    print(f"   âŒ Overfitting nghiÃªm trá»ng")

# Visualization cá»§a predictions
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.5, color='blue', label='Train')
plt.scatter(y_val, y_val_pred, alpha=0.5, color='red', label='Validation')
min_price = min(y_train.min(), y_val.min())
max_price = max(y_train.max(), y_val.max())
plt.plot([min_price, max_price], [min_price, max_price], 'k--', alpha=0.8)
plt.xlabel('GiÃ¡ thá»±c táº¿')
plt.ylabel('GiÃ¡ dá»± Ä‘oÃ¡n')
plt.title('Predicted vs Actual Prices')
plt.legend()

plt.subplot(1, 2, 2)
residuals_train = y_train - y_train_pred
residuals_val = y_val - y_val_pred
plt.scatter(y_train_pred, residuals_train, alpha=0.5, color='blue', label='Train')
plt.scatter(y_val_pred, residuals_val, alpha=0.5, color='red', label='Validation')
plt.axhline(y=0, color='k', linestyle='--', alpha=0.8)
plt.xlabel('GiÃ¡ dá»± Ä‘oÃ¡n')
plt.ylabel('Residuals (Thá»±c táº¿ - Dá»± Ä‘oÃ¡n)')
plt.title('Residual Plot')
plt.legend()

plt.tight_layout()
plt.show()

# ========================================
# BÆ¯á»šC 9: Dá»° ÄOÃN TRÃŠN TEST SET
# ========================================
print("\n" + "=" * 60)
print("ğŸ”® BÆ¯á»šC 9: Dá»± Ä‘oÃ¡n trÃªn Test Set")
print("=" * 60)

print(f"ğŸš€ ÄANG THá»°C HIá»†N Dá»° ÄOÃN...")
test_predictions = model.predict(X_test_scaled)

# Äáº£m báº£o predictions khÃ´ng Ã¢m (giÃ¡ nhÃ  khÃ´ng thá»ƒ Ã¢m)
negative_count = (test_predictions < 0).sum()
if negative_count > 0:
    print(f"   âš ï¸ PhÃ¡t hiá»‡n {negative_count} dá»± Ä‘oÃ¡n Ã¢m - Ä‘Ã£ chuyá»ƒn thÃ nh 0")
    test_predictions = np.maximum(test_predictions, 0)

print(f"   âœ… HoÃ n thÃ nh dá»± Ä‘oÃ¡n cho {len(test_predictions):,} máº«u")

# Thá»‘ng kÃª dá»± Ä‘oÃ¡n
print(f"\nğŸ“Š THá»NG KÃŠ Dá»° ÄOÃN:")
print(f"   ğŸ’° GiÃ¡ tháº¥p nháº¥t: {test_predictions.min():,.0f} JPY")
print(f"   ğŸ’° GiÃ¡ cao nháº¥t: {test_predictions.max():,.0f} JPY")
print(f"   ğŸ’° GiÃ¡ trung bÃ¬nh: {test_predictions.mean():,.0f} JPY")
print(f"   ğŸ’° GiÃ¡ trung vá»‹: {np.median(test_predictions):,.0f} JPY")
print(f"   ğŸ“ˆ Äá»™ lá»‡ch chuáº©n: {test_predictions.std():,.0f} JPY")

# So sÃ¡nh vá»›i train data
print(f"\nğŸ” SO SÃNH Vá»šI TRAIN DATA:")
print(f"   ğŸ“Š Train - GiÃ¡ trung bÃ¬nh: {y.mean():,.0f} JPY")
print(f"   ğŸ“Š Test - GiÃ¡ dá»± Ä‘oÃ¡n TB: {test_predictions.mean():,.0f} JPY")
print(f"   ğŸ“Š ChÃªnh lá»‡ch: {abs(y.mean() - test_predictions.mean()):,.0f} JPY")

# ========================================
# BÆ¯á»šC 10: Táº O SUBMISSION FILE
# ========================================
print("\n" + "=" * 60)
print("ğŸ’¾ BÆ¯á»šC 10: Táº¡o Submission File")
print("=" * 60)

# Táº¡o submission dataframe
submission = pd.DataFrame({
    'ID': test_data['ID'],
    'Price_JPY': test_predictions
})

# Kiá»ƒm tra format
print(f"ğŸ“‹ KIá»‚M TRA SUBMISSION:")
print(f"   ğŸ“Š Sá»‘ dÃ²ng: {len(submission):,}")
print(f"   ğŸ“Š Columns: {list(submission.columns)}")
print(f"   ğŸ“Š ID range: {submission['ID'].min()} - {submission['ID'].max()}")

# Hiá»ƒn thá»‹ má»™t vÃ i dÃ²ng Ä‘áº§u
print(f"\nğŸ‘€ XEM TRÆ¯á»šC SUBMISSION:")
print(submission.head())

# LÆ°u file
submission_filename = 'house_price_submission.csv'
submission.to_csv(submission_filename, index=False)

print(f"\nğŸ’¾ ÄÃƒ LUU SUBMISSION:")
print(f"   ğŸ“ File: {submission_filename}")
print(f"   âœ… Format: CSV khÃ´ng cÃ³ index")
print(f"   ğŸ“Š {len(submission):,} dá»± Ä‘oÃ¡n Ä‘Ã£ Ä‘Æ°á»£c lÆ°u")

# ========================================
# BÆ¯á»šC 11: TÃ“M Táº®T VÃ€ Káº¾T LUáº¬N
# ========================================
print("\n" + "=" * 60)
print("ğŸ¯ TÃ“M Táº®T VÃ€ Káº¾T LUáº¬N")
print("=" * 60)

print(f"\nğŸ“Š THÃ”NG TIN DATASET:")
print(f"   ğŸ  Train samples: {len(train_fe):,}")
print(f"   ğŸ§ª Test samples: {len(test_fe):,}")
print(f"   ğŸ”¢ Features sá»­ dá»¥ng: {len(feature_columns)}")
print(f"   âœ¨ Features táº¡o má»›i: {new_features_count}")

print(f"\nğŸ”§ Xá»¬ LÃ Dá»® LIá»†U:")
print(f"   âœ… Missing data Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ hoÃ n toÃ n")
print(f"   âœ… Categorical features Ä‘Ã£ Ä‘Æ°á»£c encode")
print(f"   âœ… Features Ä‘Ã£ Ä‘Æ°á»£c scaled")
print(f"   âœ… Outliers Ä‘Ã£ Ä‘Æ°á»£c xem xÃ©t")

print(f"\nğŸ¤– MODEL PERFORMANCE:")
print(f"   ğŸ“ˆ Validation RMSE: {val_rmse:,.0f} JPY")
print(f"   ğŸ“ˆ Validation MAE: {val_mae:,.0f} JPY")
print(f"   ğŸ“ˆ Validation RÂ²: {val_r2:.4f}")
print(f"   ğŸ¯ Model: Linear Regression")

print(f"\nğŸ’¡ CÃC BÆ¯á»šC TIáº¾P THEO Äá»‚ Cáº¢I THIá»†N:")
print(f"   ğŸ”® Thá»­ cÃ¡c algorithm khÃ¡c:")
print(f"      - Random Forest Regressor")
print(f"      - Gradient Boosting (XGBoost, LightGBM)")
print(f"      - Support Vector Regression")
print(f"   âš™ï¸ Feature Engineering nÃ¢ng cao:")
print(f"      - Polynomial features")
print(f"      - Interaction terms")
print(f"      - Target encoding cho categorical")
print(f"   ğŸ›ï¸ Hyperparameter tuning")
print(f"   ğŸ”„ Cross-validation Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ robust hÆ¡n")
print(f"   ğŸ“Š Feature selection Ä‘á»ƒ loáº¡i bá» features khÃ´ng quan trá»ng")

print(f"\nğŸ‰ HOÃ€N THÃ€NH PIPELINE!")
print(f"ğŸ“ File submission: {submission_filename}")
print("=" * 60)

# LÆ°u feature importance Ä‘á»ƒ tham kháº£o
feature_importance.to_csv('feature_importance.csv', index=False)
print(f"ğŸ’¾ ÄÃ£ lÆ°u feature importance vÃ o: feature_importance.csv")
